{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import constants\n",
    "import pandas  as pd\n",
    "from tqdm import tqdm\n",
    "import hashlib\n",
    "import collections\n",
    "import itertools\n",
    "import functools\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import safer\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extended = False\n",
    "constants.set_crawl_source(constants.CRAWL_SOURCE_EXTENDED if extended else constants.CRAWL_SOURCE_CHICAGO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = 2\n",
    "if not extended:\n",
    "    experiment = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.read_pickle(constants.LONG_DATA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all[\"crawl_number\"] = df_all[\"crawl_id\"].apply(lambda x: constants.CRAWL_NUMBER[x])\n",
    "df = df_all[df_all.experiment == experiment]\n",
    "df = df.sort_values(\"crawl_number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"author_id\"] = pd.concat([df[df.user_page_url.notnull()].user_page_url.str.slice(start=len(\"/user_details?userid=\")),df[df.data_hovercard_id.notnull()].data_hovercard_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "name_matches = collections.defaultdict(functools.partial(collections.defaultdict, set))\n",
    "name_matches_exact = collections.defaultdict(functools.partial(collections.defaultdict, set))\n",
    "\n",
    "matched_reviews = collections.defaultdict(functools.partial(collections.defaultdict, set))\n",
    "matched_exact_reviews = collections.defaultdict(functools.partial(collections.defaultdict, set))\n",
    "\n",
    "#Use a sample? -1 if doing a full run\n",
    "sample_size = -1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for crawl_id, crawl_df in df.groupby(\"crawl_number\"):\n",
    "    \n",
    "    print(f\"Crawl: {crawl_id}\" )\n",
    "    \n",
    "    \n",
    "    username_indexed_df = crawl_df.set_index(\"user_name\",append=True).reorder_levels([1,0]).sort_index(level=[0])\n",
    "\n",
    "    recommended_reviews = username_indexed_df[username_indexed_df.user_page_url.notnull()]\n",
    "    not_recommended_reviews = crawl_df[crawl_df.user_page_url.isnull()]\n",
    "    \n",
    "    \n",
    "    \n",
    "    if sample_size != -1:\n",
    "        not_recommended_reviews = not_recommended_reviews.sample(sample_size)\n",
    "    \n",
    "    for rowid, row in tqdm(not_recommended_reviews.iterrows(), total=len(not_recommended_reviews)):\n",
    "        user_name = row.user_name\n",
    "        user_friends = row.user_friends\n",
    "        user_photos = row.user_photos\n",
    "        user_reviews = row.user_review_count\n",
    "        user_location = row.user_location\n",
    "        user_image_url = row.user_image_url\n",
    "        \n",
    "        try:\n",
    "            name_matching_reviews = recommended_reviews.xs(user_name,level=0)\n",
    "        except KeyError:\n",
    "            continue\n",
    "        \n",
    "        m = name_matching_reviews[\n",
    "            #(name_matching_reviews.user_name == user_name) &\n",
    "            (name_matching_reviews.user_location == user_location) &\n",
    "            ((name_matching_reviews.user_friends - user_friends).abs() <= 1) &\n",
    "            ((name_matching_reviews.user_photos - user_photos).abs() <= 1) &\n",
    "            ((name_matching_reviews.user_review_count - user_reviews).abs() <= 1) &\n",
    "            (name_matching_reviews.user_image_url == user_image_url)\n",
    "        ]\n",
    "        \n",
    "        m_exact = m[\n",
    "            #(m.user_name == user_name) &\n",
    "            (m.user_location == user_location) &\n",
    "            ((m.user_friends - user_friends).abs() == 0) &\n",
    "            ((m.user_photos - user_photos).abs() == 0) &\n",
    "            ((m.user_review_count - user_reviews).abs() == 0) &\n",
    "            (m.user_image_url == user_image_url)\n",
    "        ]\n",
    "        \n",
    "        if len(m) > 0:\n",
    "            name_matches[crawl_id][row.data_hovercard_id].update(m.author_id)\n",
    "            matched_reviews[crawl_id][row.data_hovercard_id].update(list(m.index))\n",
    "        else:\n",
    "            name_matches[crawl_id][row.data_hovercard_id].update([])\n",
    "            \n",
    "        if len(m_exact) > 0:\n",
    "            name_matches_exact[crawl_id][row.data_hovercard_id].update(m_exact.author_id)\n",
    "            matched_exact_reviews[crawl_id][row.data_hovercard_id].update(list(m_exact.index))\n",
    "        else:\n",
    "            name_matches_exact[crawl_id][row.data_hovercard_id].update([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_matches.default_factory = functools.partial(collections.defaultdict, set)\n",
    "name_matches_exact.default_factory = functools.partial(collections.defaultdict, set)\n",
    "with safer.open(constants.AUTHOR_MATCH_FILE, \"wb+\", temp_file=True) as f:\n",
    "    pickle.dump([name_matches,name_matches_exact], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with safer.open(constants.AUTHOR_MATCH_FILE, \"rb\") as f:\n",
    "    name_matches,name_matches_exact = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_crawl = max(list(name_matches))\n",
    "match_series = pd.Series({author: len(name_matches[last_crawl][author]) for author in name_matches[last_crawl]})\n",
    "match_series_exact = pd.Series({author: len(name_matches_exact[last_crawl][author]) for author in name_matches_exact[last_crawl]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_series.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How many recommended reviews do filtered authors author?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(match_series,discrete=True)\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(match_series_exact,discrete=True)\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_series = (match_series != match_series_exact)\n",
    "mismatches = pd.Series(list(matches_series[matches_series].index))\n",
    "positive_matches_exact =  pd.Series(list(match_series_exact[match_series_exact > 0].index))\n",
    "all_values = pd.Series(list(match_series_exact.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mismatches), len(matches_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for author_id in df[(df.data_hovercard_id.notnull()) & (df.crawl_id == \"crawl_10\")].data_hovercard_id:\n",
    "    review_hashes = df[(df.data_hovercard_id == author_id) & (df.crawl_id == \"crawl_10\")].content_hash\n",
    "    if any(df[df.content_hash.isin(review_hashes)][\"user_page_url\"].notnull()):\n",
    "        print(\"Found\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    author_id = mismatches.sample(1).iloc[0]\n",
    "    print(author_id)\n",
    "    display(df[(df.data_hovercard_id == author_id) & (df.crawl_id == \"crawl_10\")])\n",
    "    review_hashes = df[(df.data_hovercard_id == author_id) & (df.crawl_id == \"crawl_10\")].content_hash\n",
    "    display(df[df.content_hash.isin(review_hashes)])\n",
    "    len(df[df.content_hash.isin(review_hashes)].user_image_url.unique())\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What does authorship look like during a reclassification event?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df_all = pd.read_pickle(constants.RECALSSIFICATION_DATA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"Experiment\", \"Stratum\", \"author_id\"]\n",
    "def get_experiment_and_strata(row):\n",
    "    df_rows = df_all.loc[row.members]\n",
    "    df_row = df_rows.iloc[0]\n",
    "    author_id = None\n",
    "    for key, row in df_rows.iterrows():\n",
    "        if not pd.isna(row.data_hovercard_id):\n",
    "            author_id = row.data_hovercard_id\n",
    "        else:\n",
    "            author_id = row.user_page_url\n",
    "            break\n",
    "    return pd.Series([df_row.experiment,df_row.stratum,author_id], index=names)\n",
    "\n",
    "experiment_strata = stats_df_all.progress_apply(get_experiment_and_strata,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df_all = pd.concat([stats_df_all,experiment_strata],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df = stats_df_all[stats_df_all.Experiment == experiment]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stats_df = stats_df.reset_index(level=[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "whitespace_re = re.compile(\"\\s+\")\n",
    "\n",
    "def trim(s, l=35):\n",
    "    s = whitespace_re.sub(\" \", s)\n",
    "    \n",
    "    if len(s) > l:\n",
    "        s = s[:l-3] + \"...\"\n",
    "    \n",
    "    \n",
    "    return s\n",
    "\n",
    "def display_review_history(review_history_df):\n",
    "    first_review = review_history_df.iloc[0]\n",
    "    content = trim(first_review.content)\n",
    "    date = first_review.date\n",
    "    business_id = first_review.business_id\n",
    "    \n",
    "    history = []\n",
    "    for crawl_id in constants.CRAWL_ORDER:\n",
    "        flagged = review_history_df[review_history_df.crawl_id == crawl_id].flagged\n",
    "        assert len(flagged) <= 1\n",
    "        if len(flagged) == 0:\n",
    "            history.append('_')\n",
    "        else:\n",
    "            history.append(\"R\" if not flagged.iloc[0] else \"F\")\n",
    "            \n",
    "    print(f\"{','.join(history)} | {date.strftime('%Y-%m-%d')} | {business_id} | {content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"author_id\"] = None #Set this temporarily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_hash = stats_df[stats_df.reclassification_swaps >= 2].sample(1).index[0]\n",
    "author_ids = df[df.content_hash == content_hash].author_id.unique()\n",
    "display(author_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "review_hashes = df[(df.author_id.isin(author_ids))].content_hash.unique()\n",
    "print(f\"Author identifiers: {','.join(author_ids)}\")\n",
    "for review_hash, review_df in df[df.content_hash.isin(review_hashes)].groupby(\"content_hash\"):\n",
    "    display_review_history(review_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "df[df.content_hash == content_hash]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "review_hashes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How many authors are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upu_offset = len(\"/user_details?userid=\")\n",
    "r_nr_author_id_table = {}\n",
    "nr_r_author_id_table = {}\n",
    "def get_author_ids(row):\n",
    "    r = None\n",
    "    nr = None\n",
    "    for member_id in row.members:\n",
    "        if pd.notna(df.loc[member_id].user_page_url):\n",
    "            r = df.loc[member_id].user_page_url#[upu_offset:]\n",
    "        elif pd.notna(df.loc[member_id].data_hovercard_id):\n",
    "            nr = df.loc[member_id].data_hovercard_id\n",
    "        else:\n",
    "            raise Exception()\n",
    "        if r and nr:\n",
    "            return r, nr\n",
    "    return r, nr\n",
    "for r,nr in tqdm(stats_df.progress_apply(get_author_ids,axis=1)):\n",
    "    r_nr_author_id_table[r] = nr\n",
    "    nr_r_author_id_table[nr] = r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_author_ids(row):\n",
    "    if pd.notna(row.data_hovercard_id):\n",
    "        if row.data_hovercard_id in nr_r_author_id_table:\n",
    "            row[\"user_page_url\"] = nr_r_author_id_table[row.data_hovercard_id]\n",
    "    elif pd.notna(row.user_page_url):\n",
    "        if row.user_page_url in r_nr_author_id_table:\n",
    "            row[\"data_hovercard_id\"] = r_nr_author_id_table[row.user_page_url]\n",
    "    return row\n",
    "df_fixed = df.progress_apply(fix_author_ids,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_linked = len(df_fixed[df_fixed.data_hovercard_id.notnull() & df_fixed.user_page_url.notnull()].data_hovercard_id.unique())\n",
    "num_unlinked_nr = len(df_fixed[df_fixed.data_hovercard_id.notnull() & df_fixed.user_page_url.isnull()].data_hovercard_id.unique())\n",
    "num_unlinked_r = len(df_fixed[df_fixed.data_hovercard_id.isnull() & df_fixed.user_page_url.notnull()].user_page_url.unique())\n",
    "\n",
    "print(f\"{num_linked + max(num_unlinked_r,num_unlinked_nr):,}-{num_linked + num_unlinked_r + num_unlinked_nr:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's look at this from a statistical perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all[\"rc_author_id\"] = None #reclassification author ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_hash_index = df_all.set_index(\"content_hash\",append=True,drop=False).reorder_levels([1,0]).sort_index(level=[0])\n",
    "df_all_recommended_hash_index = df_all_hash_index[df_all_hash_index.user_page_url.notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = {True: \"-\", False: \"+\", None: \"_\"}\n",
    "stats_df[stats_df.reclassification_swaps == 1].reclassification_order.apply(lambda ar: \"\".join(chars[x] for x in ar)).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df_all = stats_df_all.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df_reclass = stats_df_all[stats_df_all.progress_apply(lambda x: False if type(x.reclassification_order) is float else (True in x.reclassification_order and False in x.reclassification_order),axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%script false --no-raise-error\n",
    "author_id_map = {}\n",
    "\n",
    "for idx, row in tqdm(stats_df_reclass.iterrows(),total=len(stats_df_reclass)):\n",
    "    content_hash = row.content_hash\n",
    "    rows = df_all_recommended_hash_index.xs(content_hash,level=0)\n",
    "    first_row = rows.iloc[0]\n",
    "    upu_rows = rows[rows.user_page_url.notnull()]\n",
    "    dhi_rows = rows[rows.data_hovercard_id.notnull()]\n",
    "    if len(upu_rows) > 0 and len(dhi_rows) > 0:\n",
    "        author_id_map[dhi_rows.iloc[0]] = upu_rows.iloc[0]\n",
    "    author_id = upu_rows.iloc[0].user_page_url if len(upu_rows) > 0 else rows.iloc[0].data_hovercard_id\n",
    "    date = first_row.date\n",
    "    business_id = first_row.business_id\n",
    "    indexes = rows.index\n",
    "    stats_df_all.loc[content_hash,\"author_id\"] = author_id\n",
    "    stats_df_all.loc[content_hash,\"date\"] = date\n",
    "    stats_df_all.loc[content_hash,\"business_id\"] = business_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df_all[\"author_id\"] = stats_df_all[\"author_id\"].replace(author_id_map)\n",
    "\n",
    "def get_author_id(row):\n",
    "    if pd.isnull(row.user_page_url):\n",
    "        if row.data_hovercard_id in author_id_map:\n",
    "            return author_id_map[row.data_hovercard_id]\n",
    "        return row.data_hovercard_id\n",
    "    return row.user_page_url\n",
    "df_all[\"author_id\"] = df_all.progress_apply(get_author_id,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Authorship basic statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.groupby(\"experiment\").apply(lambda subdf: f\"Author range: {len(subdf[~subdf.author_id.isin(subdf.data_hovercard_id)].author_id.unique()):,} - {len(subdf.author_id.unique()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's get stats on those reviews that are reclassified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df = stats_df_all[stats_df_all.Experiment == experiment]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reclass_pattern(reclassification_order):\n",
    "    pattern = []\n",
    "    last_class = None\n",
    "    for classification in reclassification_order:\n",
    "        if classification is None:\n",
    "            continue  \n",
    "        if last_class != classification: #None (start) or change\n",
    "            pattern.append(classification)\n",
    "        last_class = classification\n",
    "    return pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matching = collections.defaultdict(lambda : 0)\n",
    "matching_bg = collections.defaultdict(lambda : 0)\n",
    "\n",
    "def get_matching_stats(m_dict,suffix=\"\",do_sum=False):\n",
    "    \n",
    "    #Sum everthing up if we need to\n",
    "    if do_sum:\n",
    "        m_dict = {k:sum(v) for k,v in m_dict.items()}\n",
    "    \n",
    "    #Ensure we have all the values we need\n",
    "    tf = [False,True]\n",
    "    for p in itertools.product(tf,tf):\n",
    "        if p not in m_dict: m_dict[p] = 0\n",
    "            \n",
    "    \n",
    "    rec_perc = m_dict[(False,False)] / (m_dict[(False,False)] + m_dict[(False,True)])\n",
    "    not_rec_perc = m_dict[(True,True)] / (m_dict[(True,True)] + m_dict[(True,False)])\n",
    "    return pd.Series([rec_perc,\n",
    "                    not_rec_perc],\n",
    "                    index=[f\"Recommended percentage matches{suffix}\", f\"Not recommended percentage matches{suffix}\"])\n",
    "\n",
    "reclass_count = stats_df.reclassification_order.apply(lambda x: 0 if type(x) is float else len(x)).max()\n",
    "\n",
    "def mode(series):\n",
    "    if len(series) == 0 or series.isna().all():\n",
    "        return None\n",
    "    else:\n",
    "        return series.mode().iloc[0]\n",
    "    \n",
    "def get_match_class(x,t=None,v=None,v_prev=None):\n",
    "    \"\"\"\n",
    "    matches prev and new -> 1\n",
    "    mathces prev but not new -> 2\n",
    "    not matches prev, but matches new -> 3\n",
    "    not matches prev, matches new -> 4\n",
    "    \"\"\"\n",
    "    \n",
    "    if x[t-1] is None or x[t] is None: return None\n",
    "    if x[t-1] == v_prev:\n",
    "        if x[t] == v:\n",
    "            return 1\n",
    "        else:\n",
    "            return 2\n",
    "    else:\n",
    "        if x[t] == v:\n",
    "            return 3\n",
    "        else:\n",
    "            return 4\n",
    "\n",
    "def get_review_group_stats(group):\n",
    "    results = []\n",
    "    \n",
    "    \n",
    "    l_matching = collections.defaultdict(lambda : [0 ] * reclass_count)\n",
    "    l_matching_bg = collections.defaultdict(lambda: 0)\n",
    "    \n",
    "    \n",
    "    majority_vote = [mode(group.reclassification_order.apply(lambda x: x[t])) for t in range(reclass_count)]\n",
    "    reclass_pattern = get_reclass_pattern(majority_vote)\n",
    "    \n",
    "    if len(group) >= 2:\n",
    "        #Every review the author wrote against every other review they wrote\n",
    "        for idx1, stats1 in group.iterrows():\n",
    "            #if type(stats1.reclassification_order) != list and math.isnan(stats1.reclassification_order):\n",
    "            #    continue\n",
    "            review_hash1 = stats1.content_hash\n",
    "            for review_hash2, stats2 in group.iterrows():\n",
    "                #if type(stats2.reclassification_order) != list and math.isnan(stats2.reclassification_order):\n",
    "                #    continue\n",
    "                review_hash2 = stats2.content_hash\n",
    "                if review_hash1 == review_hash2: continue\n",
    "                    \n",
    "                #\"background\" is every pairing\n",
    "                for c1,c2 in itertools.product(stats1.reclassification_order,stats2.reclassification_order):\n",
    "                    l_matching_bg[(c1,c2)] += 1\n",
    "                \n",
    "                #Normal is synchronized stepping\n",
    "                for t in range(len(stats1.reclassification_order)):\n",
    "                    c1 = stats1.reclassification_order[t]\n",
    "                    c2 = stats2.reclassification_order[t]\n",
    "                    if c1 == None or c2 == None: continue #Skip if review not present at this time\n",
    "                    l_matching[(c1,c2)][t] += 1\n",
    "                    \n",
    "        try:\n",
    "            review_matching_results = [get_matching_stats(l_matching,do_sum=True),get_matching_stats(l_matching_bg, suffix=\" (background)\")]\n",
    "            results += review_matching_results\n",
    "        except ZeroDivisionError: #Zero division means the reviews never line up, so we should chuck this result\n",
    "            index_names = [\"Recommended percentage matches\", \"Not recommended percentage matches\",\"Recommended percentage matches (background)\",\"Not recommended percentage matches (background)\", \"Reclassification pattern\", \"Recommended Follow percentage\", \"Not Recommended Follow Percentage\", \"ZDE\"]\n",
    "            results.append(pd.Series([None] * (len(index_names)-1) + [True],index=index_names))\n",
    "        \n",
    "        for idx in l_matching:\n",
    "            matching[idx] += sum(l_matching[idx])\n",
    "        for idx in l_matching_bg:\n",
    "            matching_bg[idx] += l_matching_bg[idx]\n",
    "            \n",
    "        perc_follow = []\n",
    "        perc_stay = []\n",
    "\n",
    "        #See how many reviews follow the majority\n",
    "        for t, v in enumerate(majority_vote):\n",
    "            if t == 0: #Skip first, can't follow no change\n",
    "                v_prev = majority_vote[0]\n",
    "                continue\n",
    "            elif v is not None and v_prev is not None and v != v_prev: #Make sure both are non-null\n",
    "                class_following = group.reclassification_order.apply(get_match_class,t=t,v=v,v_prev=v_prev)\n",
    "                cf_vc = class_following.value_counts()\n",
    "                \n",
    "                if (cf_vc.get(1,0) + cf_vc.get(2,0)) > 0: #Matched originally\n",
    "                    #When a change happens, how many follow the change?\n",
    "                    perc_follow.append(cf_vc.get(1,0)  / (cf_vc.get(1,0) + cf_vc.get(2,0)))\n",
    "                if (cf_vc.get(3,0) + cf_vc.get(4,0)) > 0: #Did not match originally\n",
    "                    #When a change happens, how many follow the change?\n",
    "                    perc_stay.append(cf_vc.get(3,0)  / (cf_vc.get(3,0) + cf_vc.get(4,0)))\n",
    "            v_prev = v\n",
    "\n",
    "        results.append(pd.Series([reclass_pattern, perc_follow, perc_stay], index=[\"Reclassification pattern\", \"Recommended Follow percentage\", \"Not Recommended Follow Percentage\"]))\n",
    "        \n",
    "    else:\n",
    "        return None\n",
    "        #index_names = [\"Recommended percentage matches\", \"Not recommended percentage matches\",\"Recommended percentage matches (background)\",\"Not recommended percentage matches (background)\", \"Reclassification pattern\", \"Recommended Follow percentage\", \"Not Recommended Follow Percentage\"]\n",
    "        #results.append(pd.Series([None] * len(index_names),index=index_names))\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    results_s = pd.concat(results,axis=0).rename(group.author_id.iloc[0]).to_frame().T\n",
    "    #display(results_s)\n",
    "    \n",
    "    return results_s\n",
    "    \n",
    "#review_group_stats = stats_df[(stats_df.reclassification_swaps >= 1)].groupby(\"author_id\").progress_apply(get_review_group_stats)\n",
    "rows = []\n",
    "for author_id, subdf in tqdm(stats_df[(stats_df.reclassification_swaps >= 1)].groupby(\"author_id\")):\n",
    "    rows.append(get_review_group_stats(subdf))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    review_group_stats = pd.concat(rows,axis=0)\n",
    "    \n",
    "    display(pd.concat([get_matching_stats(matching),get_matching_stats(matching_bg, suffix=\" (background)\")]))\n",
    "except:\n",
    "    print(\"Failed to build; removing bad rows\")\n",
    "    not_na_rows = list(filter(lambda x: x is not None, rows))\n",
    "    index_set = set()\n",
    "    for row in not_na_rows:\n",
    "        row_index = set(row.index)\n",
    "        intersection = (index_set & row_index)\n",
    "        if len(intersection) != 0:\n",
    "            print(intersection)\n",
    "            break\n",
    "        index_set |= row_index\n",
    "        \n",
    "    good_rows = []\n",
    "\n",
    "    for i in tqdm(range(1,len(not_na_rows))):\n",
    "        try:\n",
    "            pd.concat(good_rows + [not_na_rows[i]],axis=0)\n",
    "            good_rows.append(not_na_rows[i])\n",
    "        except:\n",
    "            print(f\"Found error {i}\")\n",
    "            display(pd.concat(good_rows,axis=0))\n",
    "            display(not_na_rows[i])\n",
    "            continue\n",
    "    review_group_stats = pd.concat(good_rows,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(len(review_group_stats[review_group_stats[\"Recommended percentage matches\"].notnull()]), review_group_stats[review_group_stats[\"Recommended percentage matches\"].notnull()][\"Recommended percentage matches\"].value_counts())\n",
    "display(review_group_stats[review_group_stats[\"Recommended percentage matches\"].notnull()].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many reviews does each author make?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vcs = stats_df.author_id.value_counts().rename(\"Number of reviews\")\n",
    "sns.lineplot(x=range(len(vcs)),y=vcs).set(xlabel='Number of authors')\n",
    "plt.xscale(\"log\")\n",
    "#plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_group_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do the reviews follow when the class changes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = review_group_stats[review_group_stats[\"Recommended percentage matches\"].notnull()].melt(value_vars = [\"Recommended percentage matches\", \"Not recommended percentage matches\",\"Recommended Follow percentage\", \"Not Recommended Follow Percentage\"])\n",
    "rfps = sum(data[data.variable == \"Recommended Follow percentage\"].value,[])\n",
    "nrfps = sum(data[data.variable == \"Not Recommended Follow Percentage\"].value,[])\n",
    "data = data.drop(data[data.variable.isin([\"Recommended Follow percentage\",\"Not Recommended Follow Percentage\"])].index)\n",
    "data = pd.concat([data,pd.DataFrame.from_records(({\"variable\": \"Recommended Follow percentage\", \"value\": rfp} for rfp in rfps))])\n",
    "data = pd.concat([data,pd.DataFrame.from_records(({\"variable\": \"Not Recommended Follow Percentage\", \"value\": rfp} for rfp in nrfps))])\n",
    "data[\"value\"] = data.value.astype(\"float\")\n",
    "sns.violinplot(x=\"value\",y=\"variable\",data=data,orient=\"h\")\n",
    "plt.savefig(\"../../graphs/author_based_recommended_matching.pdf\", bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.variable == \"Not Recommended Follow Percentage\"].value.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sns.histplot(review_group_stats[review_group_stats[\"Recommended percentage matches\"].notnull()],fill=False,kde=True)\n",
    "plt.savefig(\"../../graphs/author_based_recommended_matching\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_group_stats[\"Reclassification pattern\"].apply(lambda ar: None if ar is None else \"\".join(chars[x] for x in ar)).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df[stats_df.reclassification_order.notnull()].reclassification_order.apply(get_reclass_pattern).apply(lambda ar: \"\".join(chars[x] for x in ar)).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sns.histplot(sum(review_group_stats[review_group_stats[\"Recommended percentage matches\"].notnull()][\"Recommended Follow percentage\"],start=[]),fill=False,kde=True)\n",
    "plt.savefig(\"../../graphs/author_based_reclassification_recommended_following.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sns.histplot(sum(review_group_stats[review_group_stats[\"Recommended percentage matches\"].notnull()][\"Not Recommended Follow Percentage\"],start=[]),fill=False,kde=True)\n",
    "plt.savefig(\"../../graphs/author_based_reclassification_not_recommended_following.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Authors with at least 2 reviews, and their reviews don't match\n",
    "author_ids = review_group_stats[((review_group_stats[\"Recommended percentage matches\"].notnull()) & (review_group_stats[\"Recommended percentage matches\"] != 1)) |\n",
    "                               ((review_group_stats[\"Not recommended percentage matches\"].notnull()) & (review_group_stats[\"Not recommended percentage matches\"] != 1))].index.to_list()\n",
    "for i in range(10):\n",
    "    author_id = random.sample(author_ids,1)[0]\n",
    "    print(f\"Author: {author_id}\")\n",
    "    display(stats_df[stats_df.author_id == author_id])\n",
    "    print(\"----------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df[stats_df.author_id == \"AV8opO3Pqb7q33FbsHgvEQ\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CDF of reclassification swaps\n",
    "sns.ecdfplot(data=stats_df, x=\"date\", hue=\"reclassification_swaps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df.reclassification_swaps.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df[\"reclass_swaps_joined\"] = stats_df.reclassification_swaps.apply(lambda x: \"2+\" if x >= 2 else str(int(x)))\n",
    "swap = {k: f\"{k} ({v})\" for k,v in stats_df[\"reclass_swaps_joined\"].value_counts().to_dict().items()}\n",
    "stats_df[\"Number of reclassifications\"] = stats_df[\"reclass_swaps_joined\"].replace(swap)\n",
    "sns.ecdfplot(data=stats_df, x=\"date\", hue=\"Number of reclassifications\").set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
