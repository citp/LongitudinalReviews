{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import constants\n",
    "import pandas  as pd\n",
    "from tqdm import tqdm\n",
    "import hashlib\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constants.set_crawl_source(constants.CRAWL_SOURCE_EXTENDED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(constants.ZIPCODE_TO_STRATA, \"rb+\") as fp:\n",
    "    zc_to_strata = pickle.load(fp)\n",
    "\n",
    "zipcodes = constants.EXTENDED_ZIPCODES\n",
    "\n",
    "with open(constants.CENSUS_STRATA_DATA, \"rb+\") as fp:\n",
    "    df_strata = pd.read_pickle(fp)\n",
    "    df_strata = df_strata.loc[zipcodes]\n",
    "    \n",
    "business_df = pd.read_pickle(constants.BUSINESS_DATA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df = pd.read_pickle(constants.RECALSSIFICATION_DATA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(constants.LONG_DATA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"crawl_number\"] = df[\"crawl_id\"].apply(lambda x: constants.CRAWL_NUMBER[x])\n",
    "df = df.sort_values(\"crawl_number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"experiment\"] = df[\"experiment\"].replace({constants.CrawlExperiment.DENSITY:\"Density\",constants.CrawlExperiment.INCOME:\"Income\"})\n",
    "business_df[\"experiment\"] = business_df[\"experiment\"].replace({constants.CrawlExperiment.DENSITY:\"Density\",constants.CrawlExperiment.INCOME:\"Income\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Divide reviews by whether or not they're new for that crawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawl_max_dates = df.groupby(\"crawl_id\").date.max()\n",
    "crawl_min_dates = pd.Series({crawl_id:crawl_max_dates.loc[constants.CRAWL_ORDER[constants.CRAWL_ORDER.index(crawl_id)-1]] for crawl_id in crawl_max_dates.index})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_new(row):\n",
    "    return row.date > crawl_min_dates.loc[row.crawl_id] and row.date <= crawl_max_dates.loc[row.crawl_id]\n",
    "df[\"is_new\"] = df.progress_apply(is_new,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"recommended_discrete\"] = df[\"flagged\"].apply(lambda flagged: 0 if flagged else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=df[df.crawl_number != 0],x=\"crawl_id\",y=\"recommended_discrete\",hue=\"is_new\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Heatmap\n",
    "def get_perc_recommended(subdf):\n",
    "    vc = subdf.flagged.value_counts()\n",
    "    rec_perc = vc.loc[False] / len(subdf)\n",
    "    ci = scipy.stats.t.interval(alpha=0.95,df=len(subdf),loc=rec_perc,scale=scipy.stats.sem(subdf.flagged.apply(lambda x: 0 if x else 1)))\n",
    "    return pd.Series([rec_perc,len(subdf),ci], index=[\"recommend_percent\",\"count\",\"ci\"])\n",
    "\n",
    "heatmap_data = df.groupby([\"experiment\",\"is_new\",\"crawl_id\",\"stratum\"]).progress_apply(get_perc_recommended).reset_index()\n",
    "for experiment in [\"Density\",\"Income\"]:\n",
    "    sns.heatmap(data=heatmap_data[(heatmap_data.experiment == experiment) & ~heatmap_data.is_new].pivot(\"stratum\",\"crawl_id\",\"recommend_percent\"),vmin=0.7,vmax=1).set(title=f\"Old reviews percentage recommended ({experiment})\")\n",
    "    plt.show()\n",
    "    sns.heatmap(data=heatmap_data[(heatmap_data.experiment == experiment) & heatmap_data.is_new].pivot(\"stratum\",\"crawl_id\",\"recommend_percent\"),vmin=0.7,vmax=1).set(title=f\"New reviews percentage recommended ({experiment})\")\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "sns.heatmap(data=heatmap_data[(heatmap_data.experiment == experiment) & heatmap_data.is_new].pivot(\"Stratum1\",\"crawl_id\",\"recommend_percent\"),vmin=0.7,vmax=1).set(title=f\"New reviews percentage recommended ({experiment})\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Heatmap\n",
    "def get_perc_recommended(subdf):\n",
    "    vc = subdf.flagged.value_counts()\n",
    "    return vc.loc[False] / len(subdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.is_new].groupby(\"crawl_id\").stratum.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_data[heatmap_data.is_new]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### It looks like everything is within the margin of error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chicago "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constants.set_crawl_source(constants.CRAWL_SOURCE_CHICAGO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chicago_business_df = pd.read_pickle(constants.BUSINESS_DATA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chicago_stats_df = pd.read_pickle(constants.RECALSSIFICATION_DATA_FILE)\n",
    "chicago_stats_df = chicago_stats_df.reset_index(level=[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chicago_df = pd.read_pickle(constants.LONG_DATA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chicago_df[\"crawl_number\"] = chicago_df[\"crawl_id\"].apply(lambda x: constants.CRAWL_NUMBER[x])\n",
    "chicago_df = chicago_df.sort_values(\"crawl_number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chicago_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Divide reviews by whether or not they're new for that crawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chicago_crawl_max_dates = chicago_df.groupby(\"crawl_id\").date.max()\n",
    "chicago_crawl_min_dates = pd.Series({crawl_id:chicago_crawl_max_dates.loc[constants.CRAWL_ORDER[constants.CRAWL_ORDER.index(crawl_id)-1]] for crawl_id in chicago_crawl_max_dates.index})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_new(row):\n",
    "    return row.date > chicago_crawl_min_dates.loc[row.crawl_id] and row.date <= chicago_crawl_max_dates.loc[row.crawl_id]\n",
    "chicago_df[\"is_new\"] = chicago_df.progress_apply(is_new,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=chicago_df[chicago_df.is_new],x=\"crawl_id\",hue=\"flagged\",discrete=True,multiple=\"dodge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_data = chicago_df.groupby([\"is_new\",\"crawl_id\"]).progress_apply(get_perc_recommended).rename(\"recommend_percent\").to_frame().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chicago_df[\"notflagged\"] = ~chicago_df.flagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=chicago_df[chicago_df.crawl_id != \"crawl_10\"],x=\"crawl_id\",y=\"notflagged\",hue=\"is_new\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=heatmap_data,x=\"crawl_id\",y=\"recommend_percent\",hue=\"is_new\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chicago_crawl_max_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chicago_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
