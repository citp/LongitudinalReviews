{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import constants\n",
    "import pandas as pd\n",
    "import json\n",
    "import glob\n",
    "import itertools\n",
    "import numpy as np\n",
    "import hashlib\n",
    "import pickle\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(constants)\n",
    "\n",
    "#Change this to determine which experiment is loaded\n",
    "extended = False\n",
    "\n",
    "#Set data source\n",
    "if extended:\n",
    "    source = constants.CRAWL_SOURCE_EXTENDED\n",
    "else:\n",
    "    source = constants.CRAWL_SOURCE_CHICAGO\n",
    "constants.set_crawl_source(source)\n",
    "\n",
    "#Load experiment ID mapping\n",
    "with open(constants.ZIPCODE_TO_STRATA, \"rb+\") as fp:\n",
    "    zc_to_strata = pickle.load(fp)\n",
    "zc_to_experiment_strata_extended_f = lambda zc: zc_to_strata[zc]\n",
    "zc_to_experiment_strata_chicago_f = lambda zc: (constants.CrawlExperiment.CHICAGO, 0)\n",
    "if extended:\n",
    "    zc_to_experiment_strata_f = zc_to_experiment_strata_extended_f\n",
    "else:\n",
    "    zc_to_experiment_strata_f = zc_to_experiment_strata_chicago_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawl_ids = constants.CRAWL_ORDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_reviews(crawl_id, low_memory=True):\n",
    "    for files_dir, not_rec in [(constants.RECOMMENDED_DIR % crawl_id, False), (constants.NOT_RECOMMENDED_DIR % crawl_id, True)]:\n",
    "        for data_fn in os.listdir(files_dir):\n",
    "            \n",
    "            #All filenames should be of the format \"{zipcode}.json\"\n",
    "            zipcode = os.path.splitext(os.path.basename(data_fn))[0]\n",
    "            experiment, stratum = zc_to_experiment_strata_f(zipcode)\n",
    "\n",
    "            #Open the file\n",
    "            with open(os.path.join(files_dir,data_fn)) as f:\n",
    "                business_to_reviews = json.load(f)\n",
    "                \n",
    "            #Extract the reviews\n",
    "            for businessID,reviews in business_to_reviews.items():\n",
    "                for review in reviews:\n",
    "                    review[\"business_id\"] = businessID\n",
    "                    review[\"experiment\"] = experiment\n",
    "                    review[\"stratum\"] = stratum\n",
    "                    review[\"Stratum1\"] = constants.STRATA_COMMON_NAMES[experiment][stratum]\n",
    "                    review[\"Stratum2\"] = constants.COMBINED_STRATUM_COMMON_NAMES[stratum]\n",
    "                    review[\"crawl_id\"] = crawl_id\n",
    "                    review[\"flagged\"] = not_rec\n",
    "                    if review[\"date\"].endswith(\"Updated review\"):\n",
    "                        review[\"date\"] = review[\"date\"][:-len(\"Updated review\")]\n",
    "                        review[\"updated\"] = True\n",
    "                    if low_memory:\n",
    "                        #Only save the hash\n",
    "                        pass\n",
    "                    \n",
    "                    \n",
    "                    yield review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = {\n",
    "    \"content\": \"string\",\n",
    "    \"date\": \"datetime64\",\n",
    "    \"user_image_url\": \"string\",\n",
    "    \"user_page_url\": \"string\",\n",
    "    \"user_name\": \"string\",\n",
    "    \"user_location\": \"string\",\n",
    "    \"user_friends\": \"int64\",\n",
    "    \"user_photos\": \"int64\",\n",
    "    \"elite\": \"bool\",\n",
    "    \"business_id\": \"category\",\n",
    "    \"user_review_count\": \"int64\",\n",
    "    \"data_hovercard_id\": \"string\",\n",
    "    \"experiment\": \"category\",\n",
    "    \"stratum\": \"category\",\n",
    "}\n",
    "\n",
    "def _print(x):\n",
    "    print(x)\n",
    "    return x\n",
    "\n",
    "def get_df_for_crawl(*crawl_ids):\n",
    "    df = pd.DataFrame.from_records(itertools.chain.from_iterable((load_reviews(_print(crawl_id)) for crawl_id in crawl_ids))).astype(types)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_df_for_crawl(*crawl_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = df[df.duplicated([\"content\",\"rating\",\"crawl_id\",\"date\",\"user_image_url\",\"user_name\",\"user_location\"],keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_before_dedupe = len(df)\n",
    "df = df.drop_duplicates([\"content\",\"rating\",\"crawl_id\",\"date\",\"user_image_url\",\"user_name\",\"user_location\"],keep=\"first\")\n",
    "print(f\"Removed {reviews_before_dedupe - len(df):,} duplicate reviews, {(reviews_before_dedupe - len(df))/reviews_before_dedupe:%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stripped_text(text):\n",
    "    text = link_re.sub(\"\",text)\n",
    "    return nonalpha_re.sub(\"\",text)\n",
    "\n",
    "def hash_review_stripped(row):\n",
    "    review = row.content\n",
    "    stripped_review = get_stripped_text(review)\n",
    "    h = hashlib.sha1()\n",
    "    h.update(stripped_review.encode())\n",
    "    return h.digest()\n",
    "\n",
    "def hash_review(row):\n",
    "    review = row.content\n",
    "    business_id = row.business_id\n",
    "    h = hashlib.sha1()\n",
    "    h.update(business_id.encode())\n",
    "    h.update(review.encode())\n",
    "    return h.digest()\n",
    "\n",
    "df[\"content_hash\"] = df.progress_apply(hash_review,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(constants.LONG_DATA_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now do the businesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_json(v):\n",
    "    if type(v) == str or type(v) == int or type(v) == float or type(v) == bool or v == None:\n",
    "        return v\n",
    "    if type(v) == list:\n",
    "        new_dict = {}\n",
    "        for idx, item in enumerate(v):\n",
    "            flattened = flatten_json(item)\n",
    "            if type(flattened) == dict:\n",
    "                for ko,vo in flattened.items():\n",
    "                    new_dict[\"%d.%s\" % (idx,ko)] = vo\n",
    "            else:\n",
    "                new_dict[\"%d\" % (idx)] = flattened\n",
    "        return new_dict\n",
    "    elif type(v) == dict:\n",
    "        new_dict = {}\n",
    "        for key,value in v.items():\n",
    "            value = flatten_json(value)\n",
    "            if type(value) == dict:\n",
    "                for ko,vo in value.items():\n",
    "                    new_dict[\"%s.%s\" % (key,ko)] = vo\n",
    "            else:\n",
    "                new_dict[key] = value\n",
    "        return new_dict\n",
    "    else:\n",
    "        raise Exception(v,type(v))\n",
    "            \n",
    "\n",
    "def get_businesses(zipcodes):\n",
    "    for zipcode in zipcodes:\n",
    "        try:\n",
    "            with open(f\"{constants.BUSINESSES_DIR}/{zipcode}.json\") as f:\n",
    "                businesses = json.load(f)\n",
    "        except:\n",
    "            print(f\"Empty zipcode: {zipcode}\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            experiment, stratum = zc_to_experiment_strata_extended_f(zipcode)\n",
    "        except:\n",
    "            experiment, stratum = zc_to_experiment_strata_chicago_f(zipcode)\n",
    "            \n",
    "        for business in businesses:\n",
    "            if \"special_hours\" in business:\n",
    "                del business[\"special_hours\"]\n",
    "            if \"hours\" in business:\n",
    "                del business[\"hours\"]\n",
    "            \n",
    "            business[\"experiment\"] = experiment\n",
    "            business[\"stratum\"] = stratum\n",
    "                \n",
    "            try:\n",
    "                yield flatten_json(business)\n",
    "            except:\n",
    "                display(business)\n",
    "                raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_df = pd.DataFrame(get_businesses(constants.ZIPCODES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_df.to_pickle(constants.BUSINESS_DATA_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawled business data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # now that we have the missing data fixed...\n",
    "    with open(constants.REPLACEMENT_MASK_DATA,\"r\") as fp:\n",
    "        mask_data = json.load(fp)\n",
    "except:\n",
    "    print(\"No replacement data\")\n",
    "    mask_data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_crawled_businesses(zipcodes):\n",
    "    for crawl_id in crawl_ids:\n",
    "        if not os.path.exists(constants.BUSINESS_DATA_DIR % crawl_id):\n",
    "            print(f\"Empty crawl {crawl_id}\")\n",
    "            continue\n",
    "        for zipcode in zipcodes:\n",
    "            try:\n",
    "                with open(f\"{constants.BUSINESS_DATA_DIR % crawl_id}/{zipcode}.json\") as f:\n",
    "                    businesses = json.load(f)\n",
    "            except:\n",
    "                print(f\"Empty zipcode: {zipcode}\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                experiment, stratum = zc_to_experiment_strata_extended_f(zipcode)\n",
    "            except:\n",
    "                experiment, stratum = zc_to_experiment_strata_chicago_f(zipcode)\n",
    "\n",
    "            for business_id, business_data in businesses.items():\n",
    "                \n",
    "                if \"amenities\" in business_data:\n",
    "                    business_data[\"ammenities\"] = business_data[\"amenities\"] #we corrected a spelling error in data release. This un-corrects it\n",
    "                \n",
    "                #Try to recover\n",
    "                if (\"ammenities\" not in business_data or len(business_data[\"ammenities\"]) == 0) and business_id in mask_data:\n",
    "                    business_data = mask_data[business_id]\n",
    "                \n",
    "                #Flag\n",
    "                if \"ammenities\" not in business_data:\n",
    "                    business_data[\"ammenities\"] = []\n",
    "                    business_data[\"needs_manual_invervention\"] = True\n",
    "\n",
    "                business_data[\"num_ammenities\"] = len(business_data[\"ammenities\"])\n",
    "                \n",
    "                for ammenity in business_data[\"ammenities\"]:\n",
    "                    business_data[f\"ammenity_{ammenity['alias']}\"] = ammenity[\"isActive\"]\n",
    "                    \n",
    "                del business_data[\"ammenities\"]\n",
    "                    \n",
    "                \n",
    "                business_data[\"experiment\"] = experiment\n",
    "                business_data[\"stratum\"] = stratum\n",
    "                business_data[\"Stratum\"] = constants.STRATA_COMMON_NAMES[experiment][stratum]\n",
    "                business_data[\"crawl_id\"] = crawl_id\n",
    "                business_data[\"business_id\"] = business_id\n",
    "\n",
    "                try:\n",
    "                    yield flatten_json(business_data)\n",
    "                except:\n",
    "                    display(business_data)\n",
    "                    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawled_business_df = pd.DataFrame(get_crawled_businesses(constants.ZIPCODES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawled_business_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawled_business_df = crawled_business_df.set_index([\"crawl_id\",\"business_id\"],drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawled_business_df.to_pickle(constants.CRAWLED_BUSINESS_DATA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list(crawled_business_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawled_business_df[\"ammenity_customers_must_wear_masks\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawled_business_df[\"ammenities_need_manual_intervention\"] = crawled_business_df[\"ammenities_need_manual_intervention\"].fillna(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawled_business_df[\"num_ammenities\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawled_business_df[crawled_business_df.ammenities_need_manual_intervention].business_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_business_urls = business_df.loc[business_df.id.isin(crawled_business_df[crawled_business_df[\"num_ammenities\"] == 0].business_id)].apply(lambda row: (row.id,row.url.split(\"?\")[0]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(constants.MISSING_MASK_DATA,\"w+\") as fp:\n",
    "    json.dump({bid: url for bid, url in missing_business_urls}, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mask_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawled_business_df = None\n",
    "missing_business_urls = None\n",
    "business_df = None\n",
    "mask_data = None\n",
    "df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Typically this is the next step in processing\n",
    "%run ./Reclassification.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./Authorship.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "gby = duplicates.groupby([\"content\",\"rating\",\"crawl_id\",\"date\",\"user_image_url\",\"user_name\",\"user_location\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, group in gby:\n",
    "    if len(group.flagged.unique()) == 1: continue\n",
    "    display(name)\n",
    "    display(group)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x=\"crawl_id\",y=\"flagged\",data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.duplicated([\"content\",\"rating\",\"crawl_id\",\"date\",\"user_image_url\",\"user_name\",\"user_location\"],keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates.crawl_id.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_before_dedupe = len(df)\n",
    "df_deduped = df.drop_duplicates([\"content\",\"rating\",\"crawl_id\",\"date\",\"user_image_url\",\"user_name\",\"user_location\",\"flagged\"],keep=\"last\")\n",
    "print(f\"Removed {reviews_before_dedupe - len(df_deduped):,} duplicate reviews, {(reviews_before_dedupe - len(df_deduped))/reviews_before_dedupe:%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = df_deduped[df_deduped.duplicated([\"content\",\"rating\",\"crawl_id\",\"date\",\"user_image_url\",\"user_name\",\"user_location\"],keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates.crawl_id.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import constants\n",
    "import pandas as pd\n",
    "import json\n",
    "import glob\n",
    "import itertools\n",
    "import numpy as np\n",
    "import hashlib\n",
    "import pickle\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
